id: craft_seismic_dataflow_pipeline
learningObjectives:
  - Create a Dataflow pipeline using Apache Beam for seismic data.
hints:
  - "Review the Apache Beam documentation for pipeline creation: https://beam.apache.org/documentation/"
  - Check the schema requirements for accurate transformation logic.
  - Ensure you have the correct GCS path and BigQuery table details.
startFlow:
  do:
    - actionId: bot_message
      params:
        person: lucca
        messages:
          - text: In this task, you will create a Dataflow pipeline to process seismic data stored in Google Cloud Storage (GCS) and write the processed data to BigQuery.
          - text: "[Google Cloud Dataflow](https://cloud.google.com/products/dataflow) is a fully managed service for executing Apache Beam pipelines within the Google Cloud Platform ecosystem."
          - text: "It allows for the processing of large datasets in a distributed manner."
          - text: "Here is a Python script to help you get started:"
          - text: |
              import apache_beam as beam
              from apache_beam.options.pipeline_options import PipelineOptions
              from apache_beam.io import ReadFromText
              from apache_beam.io.gcp.bigquery import WriteToBigQuery

              class ProcessSeismicData(beam.DoFn):
                  def process(self, element):
                      print(element)
                      # Assume the data format is CSV and split by commas
                      fields = element.split(',')

                      # Extract fields according to the CSV structure
                      return [{
                          'timestamp': fields[0]
                      }]

              def run(argv=None):
                  pipeline_options = PipelineOptions()
                  with beam.Pipeline(options=pipeline_options) as p:
                      # Replace with your GCS path
                      gcs_input_path = 

                      # Replace with your BigQuery dataset and table
                      bigquery_table = 
                      (p
                       | 'ReadFromGCS' >> ReadFromText(gcs_input_path, skip_header_lines=1)
                       | 'ProcessData' >> beam.ParDo(ProcessSeismicData())
                       | 'WriteToBigQuery' >> WriteToBigQuery(
                           bigquery_table,
                           schema='timestamp:STRING, magnitude:FLOAT, depth:FLOAT, location:STRING',
                           write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE,
                           create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED
                       ))

              if __name__ == '__main__':
                  run()
          - text: Fill in `gcs_input_path` and `bigquery_table` with your details, and ensure `fields` in `ProcessSeismicData` matches your CSV structure.
          - text: "Also, in each dataset there are tables. We now want to put the records in a table called `data`."
          - text: "Once you've completed the script, let me know"
    - actionId: ready_message
      params:
        person: lucca
trigger:
  type: user_ready_response
  flowNode:
    do:
      - actionId: finish_step
